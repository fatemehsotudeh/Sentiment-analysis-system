{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "from helpers import *\n",
    "from feature_extraction import *\n",
    "from modeling import *\n",
    "from evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   Tweet ID       entity sentiment  \\\n0      2401  Borderlands  Positive   \n1      2401  Borderlands  Positive   \n2      2401  Borderlands  Positive   \n3      2401  Borderlands  Positive   \n4      2401  Borderlands  Positive   \n\n                                       Tweet content  \\\n0  im getting on borderlands and i will murder yo...   \n1  I am coming to the borders and I will kill you...   \n2  im getting on borderlands and i will kill you ...   \n3  im coming on borderlands and i will murder you...   \n4  im getting on borderlands 2 and i will murder ...   \n\n  Preprocessed Tweet content  \n0   im get borderland murder  \n1           come border kill  \n2     im get borderland kill  \n3  im come borderland murder  \n4   im get borderland murder  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweet ID</th>\n      <th>entity</th>\n      <th>sentiment</th>\n      <th>Tweet content</th>\n      <th>Preprocessed Tweet content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2401</td>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>im getting on borderlands and i will murder yo...</td>\n      <td>im get borderland murder</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2401</td>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>I am coming to the borders and I will kill you...</td>\n      <td>come border kill</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2401</td>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>im getting on borderlands and i will kill you ...</td>\n      <td>im get borderland kill</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2401</td>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>im coming on borderlands and i will murder you...</td>\n      <td>im come borderland murder</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2401</td>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>im getting on borderlands 2 and i will murder ...</td>\n      <td>im get borderland murder</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path = '../data/processed_data/'\n",
    "df_train = read_file(base_path + 'preprocessed_training_tweets.csv')\n",
    "df_test = read_file(base_path + 'preprocessed_test_tweets.csv')\n",
    "df_validation = read_file(base_path + 'preprocessed_validation_tweets.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Drop nan values (after preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_train = df_train.dropna(subset=['Preprocessed Tweet content'])\n",
    "df_test = df_test.dropna(subset=['Preprocessed Tweet content'])\n",
    "df_validation = df_validation.dropna(subset=['Preprocessed Tweet content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Text Vectorization\n",
    "- Bag of Words\n",
    "- TF-IDF\n",
    "- Word2Vec\n",
    "- GloVe\n",
    "- FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 1- Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<72310x33350 sparse matrix of type '<class 'numpy.int64'>'\n\twith 747231 stored elements in Compressed Sparse Row format>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bow, X_val_bow, X_test_bow = vectorize_bow(df_train['Preprocessed Tweet content'], df_validation['Preprocessed Tweet content'], df_test['Preprocessed Tweet content'])\n",
    "X_train_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 2- TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<72310x33350 sparse matrix of type '<class 'numpy.float64'>'\n\twith 747231 stored elements in Compressed Sparse Row format>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf, X_val_tfidf, X_test_tfidf = vectorize_tfidf(df_train['Preprocessed Tweet content'], df_validation['Preprocessed Tweet content'], df_test['Preprocessed Tweet content'])\n",
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 3- Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Word2Vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m X_train_word2vec, X_val_word2vec, X_test_word2vec \u001B[38;5;241m=\u001B[39m \u001B[43mvectorize_word2vec_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf_train\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mTweet content\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdf_validation\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mTweet content\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdf_test\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mTweet content\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m X_train_word2vec\n",
      "File \u001B[1;32m~\\projects\\Sentiment-analysis-system\\notebooks\\../scripts\\feature_extraction.py:25\u001B[0m, in \u001B[0;36mvectorize_word2vec_data\u001B[1;34m(train_data, val_data, test_data, vector_size, window, min_count, workers)\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mvectorize_word2vec_data\u001B[39m(train_data, val_data, test_data, vector_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m, window\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, min_count\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, workers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m):\n\u001B[0;32m     24\u001B[0m     tokenized_data \u001B[38;5;241m=\u001B[39m [word_tokenize(data) \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m train_data]\n\u001B[1;32m---> 25\u001B[0m     word2vec_model \u001B[38;5;241m=\u001B[39m \u001B[43mWord2Vec\u001B[49m(sentences\u001B[38;5;241m=\u001B[39mtokenized_data, vector_size\u001B[38;5;241m=\u001B[39mvector_size, window\u001B[38;5;241m=\u001B[39mwindow, min_count\u001B[38;5;241m=\u001B[39mmin_count,\n\u001B[0;32m     26\u001B[0m                               workers\u001B[38;5;241m=\u001B[39mworkers)\n\u001B[0;32m     28\u001B[0m     vectors_train \u001B[38;5;241m=\u001B[39m [word2vec_model\u001B[38;5;241m.\u001B[39mwv[token] \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m train_data \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m word_tokenize(data) \u001B[38;5;28;01mif\u001B[39;00m\n\u001B[0;32m     29\u001B[0m                      token \u001B[38;5;129;01min\u001B[39;00m word2vec_model\u001B[38;5;241m.\u001B[39mwv]\n\u001B[0;32m     30\u001B[0m     vectors_val \u001B[38;5;241m=\u001B[39m [word2vec_model\u001B[38;5;241m.\u001B[39mwv[token] \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m val_data \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m word_tokenize(data) \u001B[38;5;28;01mif\u001B[39;00m\n\u001B[0;32m     31\u001B[0m                    token \u001B[38;5;129;01min\u001B[39;00m word2vec_model\u001B[38;5;241m.\u001B[39mwv]\n",
      "\u001B[1;31mNameError\u001B[0m: name 'Word2Vec' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_word2vec, X_val_word2vec, X_test_word2vec = vectorize_word2vec_data(df_train['Tweet content'], df_validation['Tweet content'], df_test['Tweet content'])\n",
    "X_train_word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 4- GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.16185583,  0.07815364,  0.12150591, ..., -0.23421918,\n        -0.03753255,  0.14226   ],\n       [ 0.35839301,  0.08509   ,  0.13379246, ..., -0.22806492,\n        -0.10831277,  0.21318831],\n       [ 0.21978219,  0.00331909,  0.28728227, ..., -0.24701373,\n        -0.07912255,  0.17095545],\n       ...,\n       [ 0.2251264 ,  0.04810692,  0.2021436 , ..., -0.19074525,\n         0.05156472, -0.08938948],\n       [ 0.20418338,  0.03497481,  0.26873603, ..., -0.21134004,\n         0.02856181, -0.05868534],\n       [ 0.28219232,  0.1075846 ,  0.13965788, ..., -0.21725121,\n         0.02088072, -0.0263792 ]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model = load_glove_model('../models/glove.6B.50d.txt')\n",
    "X_train_glove, X_val_glove, X_test_glove = vectorize_glove(df_train['Tweet content'], df_validation['Tweet content'], df_test['Tweet content'], glove_model)\n",
    "X_train_glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Train SVM model\n",
    "\n",
    "-using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FSG\\projects\\Sentiment-analysis-system\\env\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "svm_model_bow = train_svm_model(X_train_bow, df_train['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM using Bag Of Words\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'accuracy': 0.9218436873747495,\n 'confusion_matrix': array([[ 84,   3,   0,   5],\n        [  3, 115,   3,   4],\n        [  0,   3, 126,   5],\n        [  6,   4,   3, 135]], dtype=int64),\n 'precision': 0.9218436873747495,\n 'recall': 0.9218436873747495,\n 'f1_score': 0.9218436873747495}"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_test_bow=svm_model_bow.predict(X_test_bow)\n",
    "print(\"SVM using Bag Of Words\")\n",
    "evaluate_model(svm_model_bow, df_test['sentiment'],predict_test_bow)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "-using tf-idf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    }
   ],
   "source": [
    "svm_model_tf_idf = train_svm_model(X_train_tfidf, df_train['sentiment'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM using tf-idf\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'accuracy': 0.8957915831663327,\n 'confusion_matrix': array([[ 83,   4,   0,   5],\n        [  3, 113,   4,   5],\n        [  3,   8, 116,   7],\n        [  7,   3,   3, 135]], dtype=int64),\n 'precision': 0.8957915831663327,\n 'recall': 0.8957915831663327,\n 'f1_score': 0.8957915831663327}"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_test_tf_idf=svm_model_tf_idf.predict(X_test_bow)\n",
    "print(\"SVM using tf-idf\")\n",
    "evaluate_model(svm_model_tf_idf, df_test['sentiment'],predict_test_tf_idf)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    }
   ],
   "source": [
    "svm_model_glove = train_svm_model(X_train_glove, df_train['sentiment'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM using glove\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'accuracy': 0.45691382765531063,\n 'confusion_matrix': array([[ 3, 28, 23, 38],\n        [ 0, 91, 20, 14],\n        [ 2, 47, 48, 37],\n        [ 2, 36, 24, 86]], dtype=int64),\n 'precision': 0.45691382765531063,\n 'recall': 0.45691382765531063,\n 'f1_score': 0.45691382765531063}"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_test_glove=svm_model_glove.predict(X_test_glove)\n",
    "print(\"SVM using glove\")\n",
    "evaluate_model(svm_model_glove, df_test['sentiment'],predict_test_glove)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
